import os
import zipfile
import shutil
import logging
import torch
import gdown
from pathlib import Path

from transformers import AutoTokenizer, AutoModelForCausalLM
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters

# –û—Ç–∫–ª—é—á–∞–µ–º FlexAttention
os.environ["TRANSFORMERS_NO_FLEX_ATTENTION"] = "1"

# –ó–∞–≥–ª—É—à–∫–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
if not hasattr(torch, "compiler"):
    class DummyCompiler:
        @staticmethod
        def disable(recursive=False):
            def decorator(func): return func
            return decorator
    torch.compiler = DummyCompiler()

if not hasattr(torch, "float8_e4m3fn"):
    torch.float8_e4m3fn = torch.float32

# –ü—É—Ç–∏ –∫ –º–æ–¥–µ–ª–∏
BASE_DIR = Path(__file__).resolve().parent
MODEL_DIR = BASE_DIR / "dialogpt-small"
ZIP_PATH = BASE_DIR / "dialogpt-small.zip"
TOKENIZER_JSON = MODEL_DIR / "tokenizer.json"

# ‚úÖ –°–∫–∞—á–∏–≤–∞–µ–º –∏ —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –º–æ–¥–µ–ª—å
if not MODEL_DIR.exists():
    print("\U0001F4E6 –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å —Å Google Drive...")
    file_id = "1J_uFKwD5ktNwES6SZJSdXnH5LQFxKBVH"
    url = f"https://drive.google.com/uc?id={file_id}"
    gdown.download(url, str(ZIP_PATH), quiet=False)

    print("\U0001F4C2 –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞—é –∞—Ä—Ö–∏–≤...")
    with zipfile.ZipFile(ZIP_PATH, "r") as zip_ref:
        zip_ref.extractall(BASE_DIR)

    print("\U0001F4C1 –ü–µ—Ä–µ–º–µ—â–∞—é —Ñ–∞–π–ª—ã –≤ dialogpt-small/")
    extracted_path = BASE_DIR / "dialogpt-small"
    if not extracted_path.exists():
        os.mkdir(extracted_path)
    for file in BASE_DIR.glob("*.json"):
        shutil.move(str(file), str(MODEL_DIR / file.name))
    for file in BASE_DIR.glob("*.txt"):
        shutil.move(str(file), str(MODEL_DIR / file.name))
    for file in BASE_DIR.glob("*.safetensors"):
        shutil.move(str(file), str(MODEL_DIR / file.name))
    print("‚úÖ –ú–æ–¥–µ–ª—å —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω–∞.")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ tokenizer.json
if not TOKENIZER_JSON.exists():
    raise FileNotFoundError(f"‚ùå –§–∞–π–ª {TOKENIZER_JSON} –Ω–µ –Ω–∞–π–¥–µ–Ω.")

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å
print("ü§ñ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å...")
tokenizer = AutoTokenizer.from_pretrained(str(MODEL_DIR), local_files_only=True)
model = AutoModelForCausalLM.from_pretrained(str(MODEL_DIR), local_files_only=True).to("cpu")

# –ò—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–æ–≤
chat_histories = {}

# –õ–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(format="%(asctime)s - %(levelname)s - %(message)s", level=logging.INFO)

# –ö–æ–º–∞–Ω–¥–∞ /start
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    await update.message.reply_text("–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –Ω–∞ DialoGPT. –ù–∞–ø–∏—à–∏ –º–Ω–µ —á—Ç–æ-–Ω–∏–±—É–¥—å!")

# –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    chat_id = update.effective_chat.id
    user_message = update.message.text

    new_input_ids = tokenizer.encode(user_message + tokenizer.eos_token, return_tensors="pt")

    if chat_id not in chat_histories or chat_histories[chat_id].shape[-1] > 256:
        bot_input_ids = new_input_ids
    else:
        bot_input_ids = torch.cat([chat_histories[chat_id], new_input_ids], dim=-1)

    chat_history_ids = model.generate(
        bot_input_ids,
        attention_mask=torch.ones_like(bot_input_ids),  # –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
        max_length=200,
        pad_token_id=tokenizer.eos_token_id
    )

    chat_histories[chat_id] = chat_history_ids
    response_ids = chat_history_ids[:, bot_input_ids.shape[-1]:]
    bot_response = tokenizer.decode(response_ids[0], skip_special_tokens=True)

    if bot_response.strip():
        await update.message.reply_text(bot_response)
    else:
        await update.message.reply_text("(–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç, –ø–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑)")

# –ó–∞–ø—É—Å–∫
async def main():
    token = os.environ.get("TELEGRAM_BOT_TOKEN")
    if not token:
        print("‚ùå –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è TELEGRAM_BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        return

    app = ApplicationBuilder().token(token).build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

    print("ü§ñ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω.")
    await app.run_polling()

if __name__ == "__main__":
    import nest_asyncio
    import asyncio
    nest_asyncio.apply()
    asyncio.get_event_loop().run_until_complete(main())
