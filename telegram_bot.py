import os
import zipfile
import logging
import torch
import gdown
import shutil
import asyncio
from pathlib import Path

from transformers import AutoTokenizer, AutoModelForCausalLM
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(format="%(asctime)s - %(levelname)s - %(message)s", level=logging.INFO)

# –û—Ç–∫–ª—é—á–∞–µ–º FlexAttention
os.environ["TRANSFORMERS_NO_FLEX_ATTENTION"] = "1"

# –ó–∞–≥–ª—É—à–∫–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
if not hasattr(torch, "compiler"):
    class DummyCompiler:
        @staticmethod
        def disable(recursive=False):
            def decorator(func): return func
            return decorator
    torch.compiler = DummyCompiler()
if not hasattr(torch, "float8_e4m3fn"):
    torch.float8_e4m3fn = torch.float32

# –ü—É—Ç–∏
MODEL_NAME = "dialogpt-small"
MODEL_DIR = Path(f"./{MODEL_NAME}").resolve()
ZIP_PATH = f"{MODEL_NAME}.zip"
TOKENIZER_JSON = MODEL_DIR / "tokenizer.json"

# ‚úÖ –°–∫–∞—á–∏–≤–∞–µ–º –∏ —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
if not TOKENIZER_JSON.exists():
    print("üì¶ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å —Å Google Drive...")
    url = "https://drive.google.com/uc?id=1J_uFKwD5ktNwES6SZJSdXnH5LQFxKBVH"
    gdown.download(url, ZIP_PATH, quiet=False)

    print("üìÇ –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞—é –∞—Ä—Ö–∏–≤...")
    with zipfile.ZipFile(ZIP_PATH, "r") as zip_ref:
        zip_ref.extractall("tmp_extract")

    print("üìÅ –ü–µ—Ä–µ–º–µ—â–∞—é —Ñ–∞–π–ª—ã –≤ dialogpt-small/")
    extracted_dir = Path("tmp_extract") / MODEL_NAME
    if extracted_dir.exists():
        if not MODEL_DIR.exists():
            shutil.move(str(extracted_dir), str(MODEL_DIR))
        shutil.rmtree("tmp_extract")
    else:
        raise FileNotFoundError("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–∞–ø–∫–∞ —Å –º–æ–¥–µ–ª—å—é –≤–Ω—É—Ç—Ä–∏ –∞—Ä—Ö–∏–≤–∞.")

    if not TOKENIZER_JSON.exists():
        raise FileNotFoundError(f"‚ùå –§–∞–π–ª {TOKENIZER_JSON} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
    print("‚úÖ –ú–æ–¥–µ–ª—å —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω–∞.")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
tokenizer = AutoTokenizer.from_pretrained(str(MODEL_DIR), local_files_only=True)
model = AutoModelForCausalLM.from_pretrained(str(MODEL_DIR), local_files_only=True).to("cpu")

# –ò—Å—Ç–æ—Ä–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
chat_histories = {}

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    await update.message.reply_text("–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –Ω–∞ DialoGPT. –ù–∞–ø–∏—à–∏ –º–Ω–µ —á—Ç–æ-–Ω–∏–±—É–¥—å.")

# –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    chat_id = update.effective_chat.id
    user_message = update.message.text
    new_input_ids = tokenizer.encode(user_message + tokenizer.eos_token, return_tensors="pt")

    if chat_id not in chat_histories or chat_histories[chat_id].shape[-1] > 256:
        bot_input_ids = new_input_ids
    else:
        bot_input_ids = torch.cat([chat_histories[chat_id], new_input_ids], dim=-1)

    chat_history_ids = model.generate(
        bot_input_ids,
        max_length=200,
        pad_token_id=tokenizer.eos_token_id
    )

    chat_histories[chat_id] = chat_history_ids
    response_ids = chat_history_ids[:, bot_input_ids.shape[-1]:]
    bot_response = tokenizer.decode(response_ids[0], skip_special_tokens=True)

    await update.message.reply_text(bot_response)

# –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞
async def main():
    token = os.environ.get("TELEGRAM_BOT_TOKEN")
    if not token:
        print("‚ùå –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è TELEGRAM_BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        return

    app = ApplicationBuilder().token(token).build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

    print("ü§ñ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω.")
    await app.run_polling()

# ‚úÖ –ó–∞–ø—É—Å–∫ (–±–µ–∑ asyncio.run)
if __name__ == "__main__":
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            loop.create_task(main())
        else:
            loop.run_until_complete(main())
    except RuntimeError:
        asyncio.run(main())
